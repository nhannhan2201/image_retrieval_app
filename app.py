import streamlit as st
import pandas as pd
import faiss
from PIL import Image

from utils.retrieval import load_model, query_images
from utils.ocr_search import search_ocr

st.set_page_config(page_title="Video Retrieval Demo", layout="wide")

@st.cache_resource
def load_all_data():
    # Load FAISS & CLIP
    index = faiss.read_index("data/merged_index.index")
    meta_df = pd.read_csv("data/metadata.csv")
    model, tokenizer, _ = load_model()
    
    # Load OCR data
    ocr_df = pd.read_csv("data/ocr_data.csv")
    
    return model, tokenizer, index, meta_df, ocr_df

st.title("üé• Video Retrieval Demo")
st.write("T√¨m ki·∫øm keyframe trong video b·∫±ng **CLIP** (Visual) ho·∫∑c **OCR text similarity**")

mode = st.radio("Ch·ªçn ph∆∞∆°ng th·ª©c t√¨m ki·∫øm:", ["Visual Search (CLIP)", "OCR Search (Text)"])

model, tokenizer, index, meta_df, ocr_df = load_all_data()

query = st.text_input("Nh·∫≠p truy v·∫•n:", "ng∆∞·ªùi ƒë√†n √¥ng ƒëang ph√°t bi·ªÉu")
top_k = st.slider("S·ªë l∆∞·ª£ng k·∫øt qu·∫£", 1, 10, 5)

if st.button("üîç T√¨m ki·∫øm"):
    if mode.startswith("Visual"):
        ids, distances = query_images(model, tokenizer, index, query, top_k)
        cols = st.columns(top_k)
        for i, (idx, dist) in enumerate(zip(ids, distances)):
            row = meta_df.iloc[idx]
            img = Image.open(row["full_path"])
            with cols[i]:
                st.image(img, caption=f"{row['image_id']} (score={1/(1+dist):.3f})")
    else:
        results = search_ocr(ocr_df, query, top_k)
        st.write("### üßæ K·∫øt qu·∫£ OCR:")
        for r in results:
            st.markdown(f"**[{r['similarity']:.2f}]** ‚Üí `{r['content'][:120]}...`")
